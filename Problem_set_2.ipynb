{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem set 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting the mnist dataset from keras and importing other libraries that would help in this problem statement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sources: https://www.youtube.com/watch?v=81ZGOib7DTk\n",
    "https://www.datacamp.com/community/tutorials/svm-classification-scikit-learn-python\n",
    "https://towardsdatascience.com/write-markdown-latex-in-the-jupyter-notebook-10985edb91fd, https://nlp.stanford.edu/IR-book/html/htmledition/support-vector-machines-the-linearly-separable-case-1.html#:~:text=The%20SVM%20in%20particular%20defines,the%20margin%20of%20the%20classifier.&text=A%20classifier%20with%20a%20large%20margin%20makes%20no%20low%20certainty%20classification%20decisions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TASK 1: Write code to train a multi-class support vector classifier with dot-product kernel and 1-norm\n",
    "soft margin using the MNIST training data set.Then report the performance using MNIST test data set. There is a hyper-parameter that sets the trade-off between the margin and the training error | tune this hyper-parameter through cross-validation.\n",
    "\n",
    "Linear Kernel: A linear kernel can be used as normal dot product any two given observations. The product between two vectors is the sum of the multiplication of each pair of input values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.datasets import mnist\n",
    "import math\n",
    "from sklearn import svm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the testing(10000) and training data(60000) samples, creating SVM classifier, training the model using training sets, and also predicting the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784) (60000,) (10000, 784) (10000,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lovey\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n"
     ]
    }
   ],
   "source": [
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train = x_train.reshape(60000,784) #had to reshape it to make equal dimensions \n",
    "x_test = x_test.reshape(10000,784) #had to reshape it to make equal dimensions\n",
    "print(x_train.shape, y_train.shape, x_test.shape, y_test.shape)\n",
    "clf = svm.LinearSVC()\n",
    "clf.fit(x_train, y_train) #training the data \n",
    "y_pred = clf.predict(x_test) #predict the response of test dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this the accuracy of dataset is calculated for that I am importing metrics from sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 87.26 %\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "accuracy = metrics.accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy*100, \"%\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TASK2:\n",
    "\n",
    "Identify the Lagrange dual problem of the following primal problem:\n",
    "Given Features $(x_1,y_1),....(x_N,y_N)$ $where y_1,...y_N\\in{-1,1}$ \n",
    "\\begin{equation}\n",
    "Minimize \\; w^T.w+C\\sum_{i=1}^N\\xi_i \\;\n",
    "\\\n",
    "Subject\\;to\\;y_i(w^T.x_i)\\geq1-\\xi_i and \\;\\;\\xi_i\\geq0 \n",
    "\\end{equation}\n",
    "inequality constraints can be expressed as and also lagrange form is: \n",
    "\\begin{equation}\n",
    "2-y_i(w^T.x_i)-\\xi_i\\leq0 where\\ 1-\\xi\\leq0\n",
    "\\\\\n",
    "L(w,\\alpha) = w^T.w+C\\sum_{i=1}^N\\xi_i + \\alpha_1(2-y_i(w^T.x_i)-\\xi_i)+\\alpha_2(1-\\xi)\n",
    "\\end{equation}\n",
    "where $\\alpha_1$ & $\\alpha_2$ are the Lagrange Multipliers where $\\alpha_i\\geq0$.\n",
    "Set the partial derivative function to Zero.\n",
    "\\begin{equation}\n",
    "\\partial_wL = 2w - \\alpha_1y_ix_i= 0\n",
    "\\\\\n",
    "w = \\frac{\\alpha_1y_ix_i}{2}\n",
    "\\end{equation}\n",
    "The margin is:\n",
    "\\begin{equation}\n",
    "\\gamma = y\\frac{w^Tw+b}{||w||}\n",
    "\\end{equation}\n",
    "\n",
    "The SVM in particular defines the criterion to be looking for a decision surface that is maximally far away from any data point. This distance from the decision surface to the closest data point determines the margin of the classifier.\n",
    "\n",
    "Benefits of maximizing the margin is this allows to handle minor  errors in the data sets. \n",
    "\n",
    "Solving Dual problem advantages: In the Dual problem we take $\\alpha_i$ which help us to decide the importance of the subjected features also computation is fast as well as optimzation is easy.\n",
    "\n",
    "Solving Primal Problem Disadvantages: we only obtain the optimal $w$ also Computing the scalar product of $w^Tx$ which makes the computation slow. \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
